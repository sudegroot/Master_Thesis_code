{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tweepy.client import Client\n",
    "import tweepy\n",
    "import time\n",
    "\n",
    "bearer_token = \"FILL IN\"\n",
    "API_Key = \"FILL IN\"\n",
    "API_secret_key = \"FILL IN\"\n",
    "client = Client(bearer_token = bearer_token, \n",
    "                consumer_key = API_Key,\n",
    "                consumer_secret = API_secret_key,\n",
    "                wait_on_rate_limit = False,\n",
    "               return_type = dict)\n",
    "\n",
    "# merge old and new data and save as csv\n",
    "def merge_data(new_data, old_data, filename):\n",
    "    merged_data = pd.concat([old_data, new_data])\n",
    "    merged_data.to_csv(filename, sep = \"|\")\n",
    "    return merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GET USER FOLLOWERS UKRAINE DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start time\n",
    "start_time = time.time()\n",
    "\n",
    "# initiate filename\n",
    "filename = \"users_April_Ukraine.csv\"\n",
    "\n",
    "# read data list of ids\n",
    "user_list = pd.read_csv(\"userID_list.csv\").drop(columns = ['Unnamed: 0'])\n",
    "user_list['id']\n",
    "\n",
    "# initiate start + end id\n",
    "start_id = 0\n",
    "end_id = 100\n",
    "\n",
    "# index of last user id \n",
    "last_id = len(user_list) - 1\n",
    "\n",
    "# set finished on False\n",
    "finished = False\n",
    "\n",
    "# get first users\n",
    "ids = [item for item in user_list[start_id:end_id].id]\n",
    "old_users = client.get_users(ids = ids, user_fields = [\"created_at\", \"protected\", \"public_metrics\", \"verified\"])\n",
    "old_users_csv = pd.json_normalize(old_users['data'])\n",
    "old_users_csv['username'] = old_users_csv.apply(lambda row: row['username'].replace(\"|\", \"\"), axis = 1) # remove | from username\n",
    "old_users_csv['name'] = old_users_csv.apply(lambda row: row['name'].replace(\"|\", \"\"), axis = 1) # remove | from name\n",
    "\n",
    "# start user lookup\n",
    "while finished == False:\n",
    "    \n",
    "    # update and inform about ids\n",
    "    print(\"number of ids: \", end_id)\n",
    "    start_id = start_id + 100\n",
    "    end_id = end_id + 100\n",
    "    \n",
    "    # make sure to get to the final id\n",
    "    if end_id > last_id:\n",
    "        end_id = last_id \n",
    "    \n",
    "    # make sure to stop when done\n",
    "    if start_id >= last_id:\n",
    "        finished = True\n",
    "    \n",
    "    # get 100 ids\n",
    "    ids = [item for item in user_list[start_id:end_id].id]\n",
    "    \n",
    "    # get user data\n",
    "    new_users = client.get_users(ids = ids, user_fields = [\"created_at\", \"protected\", \"public_metrics\", \"verified\"])\n",
    "    \n",
    "    # update csv\n",
    "    new_users_csv = pd.json_normalize(new_users['data'])\n",
    "    new_users_csv['username'] = new_users_csv.apply(lambda row: row['username'].replace(\"|\", \"\"), axis = 1) # remove | from username\n",
    "    new_users_csv['name'] = new_users_csv.apply(lambda row: row['name'].replace(\"|\", \"\"), axis = 1) # remove | from name\n",
    "    \n",
    "    old_users_csv = merge_data(new_users_csv, old_users_csv, filename)\n",
    "    \n",
    "    # wait 3 seconds\n",
    "    time.sleep(3)\n",
    "    \n",
    "# end timer\n",
    "end_time = time.time()\n",
    "print(\"time elapsed in minutes: \", (end_time - start_time)/60)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GET COVID DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timer\n",
    "start_timer = time.time()\n",
    "\n",
    "# initiate start and end times\n",
    "start_before_invasion = '2022-02-21T02:30:00Z'\n",
    "invasion_time = '2022-02-24T02:30:00Z'\n",
    "right_after_invasion_time = '2022-02-24T03:18:00Z'\n",
    "end_after_invasion = '2022-02-25T02:30:00Z'\n",
    "\n",
    "for item in ['before', 'after']:\n",
    "    \n",
    "    if item == 'before':\n",
    "        print(\"Scraping tweets from before the invasion\")\n",
    "        \n",
    "        # initiate start and end time\n",
    "        start_time = start_before_invasion\n",
    "        end_time = invasion_time\n",
    "        \n",
    "        # initiate filenames\n",
    "        filename_tweets = 'covid_before_tweets.csv'\n",
    "        filename_users = 'covid_before_users.csv'\n",
    "        \n",
    "    if item == 'after':\n",
    "        print(\"Scraping tweets from after the invasion\")\n",
    "        \n",
    "        # initiate start and end time\n",
    "        start_time = right_after_invasion_time\n",
    "        end_time = end_after_invasion\n",
    "        \n",
    "        # initiate filenames\n",
    "        filename_tweets = 'covid_after_tweets.csv'\n",
    "        filename_users = 'covid_after_users.csv'\n",
    "        \n",
    "    tweets = client.search_all_tweets(query = '(covid OR #covid OR #Covid OR #COVID) lang:en -is:reply -is:quote -is:retweet', expansions = [\"author_id\"], \n",
    "                           user_fields = [\"created_at\", \"public_metrics\", \"verified\"], \n",
    "                           tweet_fields = [\"author_id\", \"created_at\", \"public_metrics\", \"lang\", \"entities\", \"conversation_id\", \"referenced_tweets\"],\n",
    "                        start_time = start_time, end_time = end_time, max_results = 500)\n",
    "\n",
    "    # store dataframes\n",
    "    old_tweets = pd.json_normalize(tweets['data'])\n",
    "    old_tweets['text'] = old_tweets.apply(lambda row: row['text'].replace(\"|\", \"\"), axis = 1) # remove | from tweets\n",
    "\n",
    "    old_users = pd.json_normalize(tweets['includes']['users'])\n",
    "    old_users['username'] = old_users.apply(lambda row: row['username'].replace(\"|\", \"\"), axis = 1) # remove | from username\n",
    "    old_users['name'] = old_users.apply(lambda row: row['name'].replace(\"|\", \"\"), axis = 1) # remove | from name\n",
    "\n",
    "    # go through \n",
    "    while ('meta' in tweets):\n",
    "        \n",
    "        # break if there is no next token\n",
    "        if 'next_token' not in tweets['meta']:\n",
    "            break\n",
    "            \n",
    "        # print update\n",
    "        print(\"total number of tweets: \", len(old_tweets))\n",
    "\n",
    "        # sleep for 3 seconds to prevent reaching the rate limit\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # get next token\n",
    "        next_token = tweets['meta']['next_token']\n",
    "        tweets = client.search_all_tweets(query = '(covid OR #covid OR #Covid OR #COVID) lang:en -is:reply -is:quote -is:retweet', expansions = [\"author_id\"], \n",
    "                               user_fields = [\"created_at\", \"public_metrics\", \"verified\"], \n",
    "                               tweet_fields = [\"author_id\", \"created_at\", \"public_metrics\", \"lang\", \"entities\", \"conversation_id\", \"referenced_tweets\"],\n",
    "                            start_time = start_time, end_time = end_time, next_token = next_token, max_results = 500)\n",
    "\n",
    "        # update tweet csv\n",
    "        new_tweets = pd.json_normalize(tweets['data'])\n",
    "        new_tweets['text'] = new_tweets.apply(lambda row: row['text'].replace(\"|\", \"\"), axis = 1) # remove | from tweets\n",
    "        old_tweets = merge_data(new_tweets, old_tweets, filename_tweets)\n",
    "\n",
    "        # update users csv\n",
    "        new_users = pd.json_normalize(tweets['includes']['users'])\n",
    "        new_users['username'] = new_users.apply(lambda row: row['username'].replace(\"|\", \"\"), axis = 1) # remove | from username\n",
    "        new_users['name'] = new_users.apply(lambda row: row['name'].replace(\"|\", \"\"), axis = 1) # remove | from name\n",
    "        old_users = merge_data(new_users, old_users, filename_users)\n",
    "        \n",
    "\n",
    "# end timer\n",
    "end_time = time.time()\n",
    "print(\"time in sec elapsed: \", end_time - start_timer)\n",
    "print(\"time in min elapsed: \", (end_time - start_timer) / 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTRA COVID DATA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start timer\n",
    "start_timer = time.time()\n",
    "\n",
    "# initiate start and end times\n",
    "start_time = '2022-02-25T02:30:00Z'\n",
    "end_time = '2022-02-27T02:30:00Z'\n",
    "\n",
    "filename_tweets = 'covid_afterEXTRA_tweets.csv'\n",
    "filename_users = 'covid_afterEXTRA_users.csv'\n",
    "\n",
    "tweets = client.search_all_tweets(query = '(covid OR #covid OR #Covid OR #COVID) lang:en -is:reply -is:quote -is:retweet', expansions = [\"author_id\"], \n",
    "                           user_fields = [\"created_at\", \"public_metrics\", \"verified\"], \n",
    "                           tweet_fields = [\"author_id\", \"created_at\", \"public_metrics\", \"lang\", \"entities\", \"conversation_id\", \"referenced_tweets\"],\n",
    "                        start_time = start_time, end_time = end_time, max_results = 500)\n",
    "\n",
    "# store dataframes\n",
    "old_tweets = pd.json_normalize(tweets['data'])\n",
    "old_tweets['text'] = old_tweets.apply(lambda row: row['text'].replace(\"|\", \"\"), axis = 1) # remove | from tweets\n",
    "\n",
    "old_users = pd.json_normalize(tweets['includes']['users'])\n",
    "old_users['username'] = old_users.apply(lambda row: row['username'].replace(\"|\", \"\"), axis = 1) # remove | from username\n",
    "old_users['name'] = old_users.apply(lambda row: row['name'].replace(\"|\", \"\"), axis = 1) # remove | from name\n",
    "\n",
    "# go through \n",
    "while ('meta' in tweets):\n",
    "\n",
    "    # break if there is no next token\n",
    "    if 'next_token' not in tweets['meta']:\n",
    "        break\n",
    "\n",
    "    # print update\n",
    "    print(\"total number of tweets: \", len(old_tweets))\n",
    "\n",
    "    # sleep for 3 seconds to prevent reaching the rate limit\n",
    "    time.sleep(3)\n",
    "\n",
    "    # get next token\n",
    "    next_token = tweets['meta']['next_token']\n",
    "    tweets = client.search_all_tweets(query = '(covid OR #covid OR #Covid OR #COVID) lang:en -is:reply -is:quote -is:retweet', expansions = [\"author_id\"], \n",
    "                           user_fields = [\"created_at\", \"public_metrics\", \"verified\"], \n",
    "                           tweet_fields = [\"author_id\", \"created_at\", \"public_metrics\", \"lang\", \"entities\", \"conversation_id\", \"referenced_tweets\"],\n",
    "                        start_time = start_time, end_time = end_time, next_token = next_token, max_results = 500)\n",
    "\n",
    "    # update tweet csv\n",
    "    new_tweets = pd.json_normalize(tweets['data'])\n",
    "    new_tweets['text'] = new_tweets.apply(lambda row: row['text'].replace(\"|\", \"\"), axis = 1) # remove | from tweets\n",
    "    old_tweets = merge_data(new_tweets, old_tweets, filename_tweets)\n",
    "\n",
    "    # update users csv\n",
    "    new_users = pd.json_normalize(tweets['includes']['users'])\n",
    "    new_users['username'] = new_users.apply(lambda row: row['username'].replace(\"|\", \"\"), axis = 1) # remove | from username\n",
    "    new_users['name'] = new_users.apply(lambda row: row['name'].replace(\"|\", \"\"), axis = 1) # remove | from name\n",
    "    old_users = merge_data(new_users, old_users, filename_users)\n",
    "\n",
    "\n",
    "# end timer\n",
    "end_time = time.time()\n",
    "print(\"time in sec elapsed: \", end_time - start_timer)\n",
    "print(\"time in min elapsed: \", (end_time - start_timer) / 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
