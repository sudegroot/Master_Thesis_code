{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from functions_thesis import preprocessing, get_f1_macro, cross_validation_train, best_resampling\n",
    "from sklearn.inspection import permutation_importance\n",
    "import time\n",
    "import collections\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy import stats\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "def get_F1_distribution(testset, model):\n",
    "    F1_scores = list()\n",
    "    n = len(testset)\n",
    "    for i in np.arange(1000):\n",
    "        # get random 1000 datapoints \n",
    "        sample = testset.sample(n = n, replace = True)\n",
    "\n",
    "        X = sample[['verified', 'log_followers',\n",
    "           'log_following', 'log_tweetcount',\n",
    "           'log_listed', 'account_age_y', \n",
    "           'sex_generalized', 'tweet_char_len', \n",
    "            'hashtag_count',\n",
    "           'mention_count', 'urls_count', 'organization', 'sentiment', 'emoji_count']]\n",
    "        Y_true = sample['viral']\n",
    "\n",
    "        # make prediction\n",
    "        Y_pred = model.predict(X)\n",
    "\n",
    "        # get F1 macro and append to list\n",
    "        F1_scores.append(f1_score(Y_true, Y_pred, average = 'macro'))\n",
    "\n",
    "    return F1_scores\n",
    "\n",
    "# make dict to store all f1 distributions\n",
    "all_F1 = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORT AND PREPROCESS TEST SETS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_before = pd.read_csv(\"test_before_prep.csv\", sep = \"|\").drop(columns = ['Unnamed: 0'])\n",
    "test_after = pd.read_csv(\"test_after_prep.csv\", sep = \"|\").drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "test_before = preprocessing(test_before)\n",
    "test_after = preprocessing(test_after)\n",
    "\n",
    "print(test_before.shape, test_after.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "b_X_test = test_before[['verified', 'log_followers',\n",
    "       'log_following', 'log_tweetcount',\n",
    "       'log_listed', 'account_age_y', \n",
    "       'sex_generalized', 'tweet_char_len', \n",
    "        'hashtag_count',\n",
    "       'mention_count', 'urls_count', 'organization', 'sentiment', 'emoji_count']]\n",
    "\n",
    "b_Y_test = test_before['viral']\n",
    "print(Counter(b_Y_test))\n",
    "\n",
    "# combine data again\n",
    "before_test = b_X_test\n",
    "before_test['viral'] = b_Y_test\n",
    "print(len(before_test))\n",
    "before_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly downsample the after testset to match the same number of instances of non-viral and viral tweets of the before test set. This way, performance metrics are comparable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly undersample data\n",
    "sample = Counter(b_Y_test)\n",
    "resample = RandomUnderSampler(random_state = 42, sampling_strategy = sample)\n",
    "# prepare data\n",
    "a_X_test = test_after[['verified', 'log_followers',\n",
    "       'log_following', 'log_tweetcount',\n",
    "       'log_listed', 'account_age_y', \n",
    "       'sex_generalized', 'tweet_char_len', \n",
    "        'hashtag_count',\n",
    "       'mention_count', 'urls_count', 'organization', 'sentiment', 'emoji_count']]\n",
    "\n",
    "a_Y_test = test_after['viral']\n",
    "print(Counter(a_Y_test))\n",
    "a_X_test, a_Y_test = resample.fit_resample(a_X_test, a_Y_test)\n",
    "print(Counter(a_Y_test))\n",
    "\n",
    "# combine data again\n",
    "after_test = a_X_test\n",
    "after_test['viral'] = a_Y_test\n",
    "print(len(after_test))\n",
    "after_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_Y_test = pd.concat([b_Y_test,a_Y_test]).reset_index().drop(columns = ['index'])['viral']\n",
    "comb_X_test = pd.concat([b_X_test,a_X_test]).reset_index().drop(columns = ['index'])\n",
    "\n",
    "# randomly undersample data\n",
    "sample = Counter(b_Y_test)\n",
    "resample = RandomUnderSampler(random_state = 42, sampling_strategy = sample)\n",
    "comb_X_test, comb_Y_test = resample.fit_resample(comb_X_test, comb_Y_test)\n",
    "print(Counter(comb_Y_test))\n",
    "\n",
    "# combine data again\n",
    "comb_test = comb_X_test\n",
    "comb_test['viral'] = comb_Y_test\n",
    "print(len(comb_test))\n",
    "comb_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_x__ = b_X_test\n",
    "b_x__['after'] = 0\n",
    "a_x__ = a_X_test\n",
    "a_x__['after'] = 1\n",
    "\n",
    "COMB_Y_test = pd.concat([b_Y_test,a_Y_test]).reset_index().drop(columns = ['index'])['viral']\n",
    "COMB_X_test = pd.concat([b_x__,a_x__]).reset_index().drop(columns = ['index'])\n",
    "\n",
    "# randomly undersample data\n",
    "sample = Counter(b_Y_test)\n",
    "resample = RandomUnderSampler(random_state = 42, sampling_strategy = sample)\n",
    "COMB_X_test, COMB_Y_test = resample.fit_resample(COMB_X_test, COMB_Y_test)\n",
    "print(Counter(COMB_Y_test))\n",
    "\n",
    "# combine data again\n",
    "COMB_test = COMB_X_test\n",
    "COMB_test['viral'] = COMB_Y_test\n",
    "print(len(COMB_test))\n",
    "COMB_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEFORE DATA - FINAL MODEL + TESTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "before_data = pd.read_csv(\"before_train_val.csv\", sep = \"|\").drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "# do some preprocessing\n",
    "before_data = preprocessing(before_data)\n",
    "print(before_data.shape)\n",
    "before_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample data based on bound of 25\n",
    "print('number of train data: ', len(before_data))\n",
    "resample_data = before_data[(before_data['public_metrics.retweet_count'] < 25) | (before_data['public_metrics.retweet_count'] > 100)]\n",
    "print('after resampling with bound of 25: ', len(resample_data))\n",
    "\n",
    "# prepare data\n",
    "b_X = resample_data[['verified', 'log_followers',\n",
    "       'log_following', 'log_tweetcount',\n",
    "       'log_listed', 'account_age_y', \n",
    "       'sex_generalized', 'tweet_char_len', \n",
    "        'hashtag_count',\n",
    "       'mention_count', 'urls_count', 'organization', 'sentiment', 'emoji_count']]\n",
    "\n",
    "b_Y = resample_data['viral']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit final model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "RF = RandomForestClassifier(criterion = 'entropy', max_depth = 40, max_features = 'sqrt', min_samples_split = 5, n_estimators = 150, random_state = 42, n_jobs = 3)\n",
    "\n",
    "model_before = RF.fit(b_X, b_Y)\n",
    "print(\"done fitting the model\")\n",
    "imp_before = permutation_importance(model_before, b_X, b_Y, n_repeats = 30, random_state = 42, scoring = 'f1_macro', n_jobs = 3)\n",
    "\n",
    "print(\"number of minutes running: \", (start_time - time.time())/60)\n",
    "\n",
    "importances_before = collections.defaultdict(list)\n",
    "features = b_X.keys()\n",
    "for item in imp_before:\n",
    "            if item == 'importances':\n",
    "                for feature, importance in zip(features, imp_before[item]):\n",
    "                    importances_before[feature].append(importance)       \n",
    "\n",
    "importances_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_before_test = permutation_importance(model_before, b_X_test.drop(columns = ['viral']), b_Y_test, n_repeats = 30, random_state = 42, scoring = 'f1_macro', n_jobs = 3)\n",
    "\n",
    "print(\"number of minutes running: \", (start_time - time.time())/60)\n",
    "\n",
    "importances_before_test = collections.defaultdict(list)\n",
    "features = b_X_test.drop(columns = ['viral']).keys()\n",
    "for item in imp_before_test:\n",
    "            if item == 'importances':\n",
    "                for feature, importance in zip(features, imp_before_test[item]):\n",
    "                    importances_before_test[feature].append(importance)       \n",
    "\n",
    "importances_before_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get F1 distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_distr_before = get_F1_distribution(before_test, model_before)\n",
    "plt.hist(F1_distr_before, bins = 50)\n",
    "print(\"number of f-measures: \", len(F1_distr_before))\n",
    "print(np.mean(F1_distr_before), np.std(F1_distr_before), stats.sem(F1_distr_before))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_distr_before_CON = get_F1_distribution(after_test, model_before)\n",
    "plt.hist(F1_distr_before_CON, bins = 50)\n",
    "print(\"number of f-measures: \", len(F1_distr_before_CON))\n",
    "print(np.mean(F1_distr_before_CON), np.std(F1_distr_before_CON), stats.sem(F1_distr_before_CON))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(b_Y_test))\n",
    "true_Y = b_Y_test\n",
    "pred_Y = model_before.predict(b_X_test.drop(columns = ['viral']))\n",
    "print(f1_score(true_Y, pred_Y, average = 'macro'))\n",
    "\n",
    "cm = confusion_matrix(true_Y, pred_Y, labels=model_before.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_before.classes_)\n",
    "disp.plot()\n",
    "plt.title(\"Confusion Matrix Before Model - Before Test\")\n",
    "plt.show()\n",
    "# plt.savefig(\"conf_matrix_before.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_F1['BB'] = F1_distr_before\n",
    "all_F1['BA'] = F1_distr_before_CON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AFTER DATA - FINAL MODEL + TESTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "after_data = pd.read_csv(\"after_train_val.csv\", sep = \"|\").drop(columns = ['Unnamed: 0'])\n",
    "\n",
    "# do some preprocessing\n",
    "after_data = preprocessing(after_data)\n",
    "print(after_data.shape)\n",
    "after_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample data based on bound of 25\n",
    "print('number of train data: ', len(after_data))\n",
    "# randomly undersample data\n",
    "sample = {0: 586497, 1: 6376}\n",
    "resample = RandomUnderSampler(random_state = 42, sampling_strategy = sample)\n",
    "# prepare data\n",
    "a_X = after_data[['verified', 'log_followers',\n",
    "       'log_following', 'log_tweetcount',\n",
    "       'log_listed', 'account_age_y', \n",
    "       'sex_generalized', 'tweet_char_len', \n",
    "        'hashtag_count',\n",
    "       'mention_count', 'urls_count', 'organization', 'sentiment', 'emoji_count', 'public_metrics.retweet_count']]\n",
    "\n",
    "a_Y = after_data['viral']\n",
    "print(Counter(a_Y))\n",
    "a_X, a_Y = resample.fit_resample(a_X, a_Y)\n",
    "after_data_RUS = a_X.reset_index()\n",
    "after_data_RUS['viral'] = a_Y\n",
    "\n",
    "print('number of train data after RUS:', len(after_data_RUS))\n",
    "\n",
    "resample_data_after = after_data_RUS[(after_data_RUS['public_metrics.retweet_count'] < 10) | (after_data_RUS['public_metrics.retweet_count'] > 100)]\n",
    "print('after resampling with bound of 10: ', len(resample_data_after))\n",
    "\n",
    "a_X = resample_data_after[['verified', 'log_followers',\n",
    "       'log_following', 'log_tweetcount',\n",
    "       'log_listed', 'account_age_y', \n",
    "       'sex_generalized', 'tweet_char_len', \n",
    "        'hashtag_count',\n",
    "       'mention_count', 'urls_count', 'organization', 'sentiment', 'emoji_count']]\n",
    "\n",
    "a_Y = resample_data_after['viral']\n",
    "\n",
    "print(a_X.shape, Counter(a_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_after_test = permutation_importance(model_after, a_X_test.drop(columns = ['viral']), a_Y_test, n_repeats = 30, random_state = 42, scoring = 'f1_macro', n_jobs = 3)\n",
    "\n",
    "print(\"number of minutes running: \", (start_time - time.time())/60)\n",
    "\n",
    "importances_after_test = collections.defaultdict(list)\n",
    "features = a_X_test.drop(columns = ['viral']).keys()\n",
    "for item in imp_after_test:\n",
    "            if item == 'importances':\n",
    "                for feature, importance in zip(features, imp_after_test[item]):\n",
    "                    importances_after_test[feature].append(importance)       \n",
    "\n",
    "importances_after_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit final model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "RF = RandomForestClassifier(criterion = 'gini', max_depth = 20, max_features = 'sqrt', min_samples_split = 2, n_estimators = 150, random_state = 42, n_jobs = 3)\n",
    "model_after = RF.fit(a_X, a_Y)\n",
    "print(\"done fitting model\")\n",
    "\n",
    "imp_after = permutation_importance(model, a_X, a_Y, n_repeats = 30, random_state = 42, scoring = 'f1_macro', n_jobs = 3)\n",
    "\n",
    "print(\"number of minutes running: \", (start_time - time.time())/60)\n",
    "\n",
    "importances_after = collections.defaultdict(list)\n",
    "features = a_X.keys()\n",
    "for item in imp_after:\n",
    "            if item == 'importances':\n",
    "                for feature, importance in zip(features, imp_after[item]):\n",
    "                    importances_after[feature].append(importance)       \n",
    "\n",
    "importances_after"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get F1 distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1_distr_after = get_F1_distribution(after_test, model_after)\n",
    "plt.hist(F1_distr_after, bins = 50)\n",
    "print(\"number of f-measures: \", len(F1_distr_after))\n",
    "print(np.mean(F1_distr_after), np.std(F1_distr_after), stats.sem(F1_distr_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1_distr_after_CON = get_F1_distribution(before_test, model_after)\n",
    "plt.hist(F1_distr_after_CON, bins = 50)\n",
    "print(\"number of f-measures: \", len(F1_distr_after_CON))\n",
    "print(np.mean(F1_distr_after_CON), np.std(F1_distr_after_CON), stats.sem(F1_distr_after_CON))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(a_Y_test))\n",
    "true_Y = a_Y_test\n",
    "pred_Y = model_after.predict(a_X_test.drop(columns = ['viral']))\n",
    "print(f1_score(true_Y, pred_Y, average = 'macro'))\n",
    "\n",
    "fig = plt.figure()\n",
    "cm = confusion_matrix(true_Y, pred_Y, labels=model_after.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model_after.classes_)\n",
    "fig = disp.plot(ax = ax1)\n",
    "plt.title(\"Confusion Matrix After Model - After Test\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import plot_confusion_matrix\n",
    "sns.reset_defaults()\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (15,4))\n",
    "\n",
    "plot_confusion_matrix(model_before, b_X_test.drop(columns = ['viral', 'after']), b_Y_test, ax = ax1)\n",
    "ax1.set_title(\"1. Before model\", weight = 'bold', fontsize = 13, loc = 'left')\n",
    "plot_confusion_matrix(model_after, a_X_test.drop(columns = ['viral', 'after']), a_Y_test, ax = ax2)\n",
    "ax2.set_title(\"2. After model\", weight = 'bold', fontsize = 13, loc = 'left')\n",
    "plot_confusion_matrix(model_comb, comb_X_test.drop(columns = ['viral']), comb_Y_test, ax = ax3)\n",
    "ax3.set_title(\"3. Ignorant model\", weight = 'bold', fontsize = 13, loc = 'left')\n",
    "fig.savefig(\"confusion_matrices.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_F1['AA'] = F1_distr_after\n",
    "all_F1['AB'] = F1_distr_after_CON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**COMBINE FEATURE IMPORTANCE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_Y = b_Y\n",
    "pred_Y = model_before.predict(b_X)\n",
    "f1_b = f1_score(true_Y, pred_Y, average = 'macro')\n",
    "print(\"f1 score before = \", f1_b)\n",
    "\n",
    "true_Y = a_Y\n",
    "pred_Y = model_after.predict(a_X)\n",
    "f1_a = f1_score(true_Y, pred_Y, average = 'macro')\n",
    "print(\"f1 score after = \", f1_a)\n",
    "\n",
    "importances_before_new = dict()\n",
    "\n",
    "for item in importances_before:\n",
    "    importances_before_new[item] = importances_before[item][0] / f1_b * 100\n",
    "    \n",
    "importances_after_new = dict()\n",
    "\n",
    "for item in importances_after:\n",
    "    importances_after_new[item] = importances_after[item][0] / f1_a * 100\n",
    "    \n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize = (12,12))\n",
    "ax1.boxplot(pd.DataFrame.from_dict(importances_before_new), vert = False, labels = importances_before.keys())\n",
    "ax1.set_title(\"1. Feature importance before model - train set\", weight = 'bold', fontsize = 13, loc = 'right')\n",
    "ax1.set_xlabel(\"% drop in macro F1\", fontsize = 12)\n",
    "ax1.set_xlim([-1, 50])\n",
    "\n",
    "ax2.boxplot(pd.DataFrame.from_dict(importances_after_new), vert = False, labels = importances_after.keys())\n",
    "ax2.set_title(\"2. Feature importance after model - train set\", weight = 'bold', fontsize = 13, loc = 'right')\n",
    "ax2.set_xlabel(\"% drop in macro F1\", fontsize = 12)\n",
    "ax2.set_xlim([-1, 50])\n",
    "fig.savefig(\"Feature_Importance_train.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_perm_imp = pd.DataFrame.from_dict(importances_before_new).subtract(pd.DataFrame.from_dict(importances_after_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(diff_perm_imp, vert = False, labels = diff_perm_imp.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_Y = b_Y_test\n",
    "pred_Y = model_before.predict(b_X_test.drop(columns = ['viral', 'after']))\n",
    "f1_b = f1_score(true_Y, pred_Y, average = 'macro')\n",
    "print(\"f1 score before = \", f1_b)\n",
    "\n",
    "true_Y = a_Y_test\n",
    "pred_Y = model_after.predict(a_X_test.drop(columns = ['viral', 'after']))\n",
    "f1_a = f1_score(true_Y, pred_Y, average = 'macro')\n",
    "print(\"f1 score after = \", f1_a)\n",
    "\n",
    "importances_before_test_new = dict()\n",
    "\n",
    "for item in importances_before_test:\n",
    "    importances_before_test_new[item] = importances_before_test[item][0] / f1_b * 100\n",
    "    \n",
    "importances_after_test_new = dict()\n",
    "\n",
    "for item in importances_after_test:\n",
    "    importances_after_test_new[item] = importances_after_test[item][0] / f1_a * 100\n",
    "    \n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize = (12,12))\n",
    "ax1.boxplot(pd.DataFrame.from_dict(importances_before_test_new), vert = False, labels = importances_before_test.keys())\n",
    "ax1.set_title(\"A. Feature importance before model - test set\", weight = 'bold', fontsize = 13, loc = 'right')\n",
    "ax1.set_xlim([-1, 35])\n",
    "ax1.set_xlabel(\"% drop in macro F1\", fontsize = 12)\n",
    "\n",
    "ax2.boxplot(pd.DataFrame.from_dict(importances_after_test_new), vert = False, labels = importances_after_test.keys())\n",
    "ax2.set_title(\"B. Feature importance after model - test set\", weight = 'bold', fontsize = 13, loc = 'right')\n",
    "ax2.set_xlim([-1, 35])\n",
    "ax2.set_xlabel(\"% drop in macro F1\", fontsize = 12)\n",
    "\n",
    "fig.savefig(\"Feature_Importance_test.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "ax1.set_title(\"1. Hierarchical clustering - before train set\", weight = 'bold', loc = 'left', fontsize = 13)\n",
    "ax2.set_title(\"2. Correlation matrix - before train set\", weight = 'bold', loc = 'left', fontsize = 13)\n",
    "\n",
    "corr = spearmanr(b_X).correlation\n",
    "print(corr)\n",
    "ax2 = sns.heatmap(corr, xticklabels = b_X.keys(), yticklabels = b_X.keys())\n",
    "ax2.set_xticklabels(\n",
    "    ax2.get_xticklabels(),\n",
    "    horizontalalignment='right')\n",
    "# Ensure the correlation matrix is symmetric\n",
    "corr = (corr + corr.T) / 2\n",
    "np.fill_diagonal(corr, 1)\n",
    "\n",
    "# We convert the correlation matrix to a distance matrix before performing\n",
    "# hierarchical clustering using Ward's linkage.\n",
    "distance_matrix = 1 - np.abs(corr)\n",
    "dist_linkage = hierarchy.ward(squareform(distance_matrix))\n",
    "dendro = hierarchy.dendrogram(dist_linkage, labels=list(b_X.keys()), ax=ax1, leaf_rotation=90)\n",
    "dendro_idx = np.arange(0, len(dendro[\"ivl\"]))\n",
    "\n",
    "# ax2.imshow(corr[dendro[\"leaves\"], :][:, dendro[\"leaves\"]])\n",
    "\n",
    "# ax2.set_xticks(dendro_idx)\n",
    "# ax2.set_yticks(dendro_idx)\n",
    "# ax2.set_xticklabels(dendro[\"ivl\"], rotation=\"vertical\")\n",
    "# ax2.set_yticklabels(dendro[\"ivl\"])\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"corr_before.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 8))\n",
    "ax1.set_title(\"1. Hierarchical clustering - after train set\", weight = 'bold', loc = 'left', fontsize = 13)\n",
    "ax2.set_title(\"2. Correlation matrix - after train set\", weight = 'bold', loc = 'left', fontsize = 13)\n",
    "\n",
    "corr = spearmanr(a_X.drop(columns = ['viral'])).correlation\n",
    "print(corr)\n",
    "ax2 = sns.heatmap(corr, xticklabels = a_X.drop(columns = ['viral']).keys(), yticklabels = a_X.drop(columns = ['viral']).keys())\n",
    "ax2.set_xticklabels(\n",
    "    ax2.get_xticklabels(),\n",
    "    horizontalalignment='right')\n",
    "# Ensure the correlation matrix is symmetric\n",
    "corr = (corr + corr.T) / 2\n",
    "np.fill_diagonal(corr, 1)\n",
    "\n",
    "# We convert the correlation matrix to a distance matrix before performing\n",
    "# hierarchical clustering using Ward's linkage.\n",
    "distance_matrix = 1 - np.abs(corr)\n",
    "dist_linkage = hierarchy.ward(squareform(distance_matrix))\n",
    "dendro = hierarchy.dendrogram(dist_linkage, labels=list(a_X.drop(columns = ['viral']).keys()), ax=ax1, leaf_rotation=90)\n",
    "dendro_idx = np.arange(0, len(dendro[\"ivl\"]))\n",
    "\n",
    "# ax2.imshow(corr[dendro[\"leaves\"], :][:, dendro[\"leaves\"]])\n",
    "\n",
    "# ax2.set_xticks(dendro_idx)\n",
    "# ax2.set_yticks(dendro_idx)\n",
    "# ax2.set_xticklabels(dendro[\"ivl\"], rotation=\"vertical\")\n",
    "# ax2.set_yticklabels(dendro[\"ivl\"])\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"corr_after.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEFORE AND AFTER DATA COMBINED**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(after_data_RUS), len(before_data))\n",
    "\n",
    "after = after_data_RUS[['verified', 'log_followers', 'log_following', 'log_tweetcount',\n",
    "       'log_listed', 'account_age_y', 'sex_generalized', 'tweet_char_len',\n",
    "       'hashtag_count', 'mention_count', 'urls_count', 'organization',\n",
    "       'sentiment', 'emoji_count', 'public_metrics.retweet_count', 'viral']]\n",
    "\n",
    "\n",
    "\n",
    "before = before_data[['verified', 'log_followers', 'log_following', 'log_tweetcount',\n",
    "       'log_listed', 'account_age_y', 'sex_generalized', 'tweet_char_len',\n",
    "       'hashtag_count', 'mention_count', 'urls_count', 'organization',\n",
    "       'sentiment', 'emoji_count', 'public_metrics.retweet_count', 'viral']]\n",
    "\n",
    "combined_train = pd.concat([after, before])\n",
    "print(combined_train.shape)\n",
    "\n",
    "comb_X = combined_train[['verified', 'log_followers', 'log_following', 'log_tweetcount',\n",
    "       'log_listed', 'account_age_y', 'sex_generalized', 'tweet_char_len',\n",
    "       'hashtag_count', 'mention_count', 'urls_count', 'organization',\n",
    "       'sentiment', 'emoji_count', 'public_metrics.retweet_count']]\n",
    "\n",
    "comb_Y = combined_train['viral']\n",
    "\n",
    "sample = {0: 586497, 1: 6376}\n",
    "resample = RandomUnderSampler(random_state = 42, sampling_strategy = sample)\n",
    "comb_X, comb_Y = resample.fit_resample(comb_X, comb_Y)\n",
    "Counter(comb_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model \n",
    "model = RandomForestClassifier(n_estimators = 100, random_state = 42, n_jobs = 3)\n",
    "\n",
    "resampling_methods = {'bound' : list(np.arange(0, 65, 5))}\n",
    "scaler = False\n",
    "\n",
    "best_scores_mean_RF_comb, best_scores_std_RF_comb, best_ratio_RF_comb = best_resampling(model, comb_X, comb_Y, resampling_methods, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "ind_bound = comb_X.index[(comb_X['public_metrics.retweet_count'] >= 25) & (comb_X['public_metrics.retweet_count'] <= 100)].tolist()\n",
    "len(ind_bound)\n",
    "\n",
    "skf = StratifiedKFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "skf.get_n_splits(comb_X, comb_Y)\n",
    "\n",
    "cv = list()\n",
    "\n",
    "for item in skf.split(comb_X, comb_Y):\n",
    "    cv.append([np.array(list((set(item[0]) - set(ind_bound)))), item[1]])\n",
    "\n",
    "\n",
    "X_ = comb_X.drop(columns = ['public_metrics.retweet_count'])\n",
    "    \n",
    "# do grid search TEST\n",
    "\n",
    "model = RandomForestClassifier(random_state = 42, n_jobs = 2)\n",
    "\n",
    "grid = {\"n_estimators\" : [90, 100, 130], \n",
    "        \"criterion\" : ['gini', 'entropy'],\n",
    "        \"max_depth\" : [5, 10, 20, 40, 'None'],\n",
    "        \"min_samples_split\" : [2, 5, 10], \n",
    "        \"max_features\" : ['sqrt', 'None']}\n",
    "\n",
    "grid_search = GridSearchCV(estimator = model, param_grid = grid, n_jobs = 2, cv = cv, scoring = 'f1_macro', refit = False)\n",
    "grid_result = grid_search.fit(X_, comb_Y)\n",
    "\n",
    "mean = pd.DataFrame(grid_result.cv_results_).iloc[grid_result.best_index_]['mean_test_score']\n",
    "std = pd.DataFrame(grid_result.cv_results_).iloc[grid_result.best_index_]['std_test_score']\n",
    "\n",
    "print(\"mean score: %f +- %f\" % (mean, std))\n",
    "print(\"best parameters: \", grid_result.best_params_)\n",
    "\n",
    "# mean score: 0.735699 +- 0.003984\n",
    "# best parameters:  {'criterion': 'entropy', 'max_depth': 40, 'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 130}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit final model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(after_data_RUS), len(before_data))\n",
    "\n",
    "after = after_data_RUS[['verified', 'log_followers', 'log_following', 'log_tweetcount',\n",
    "       'log_listed', 'account_age_y', 'sex_generalized', 'tweet_char_len',\n",
    "       'hashtag_count', 'mention_count', 'urls_count', 'organization',\n",
    "       'sentiment', 'emoji_count', 'public_metrics.retweet_count', 'viral']]\n",
    "\n",
    "before = before_data[['verified', 'log_followers', 'log_following', 'log_tweetcount',\n",
    "       'log_listed', 'account_age_y', 'sex_generalized', 'tweet_char_len',\n",
    "       'hashtag_count', 'mention_count', 'urls_count', 'organization',\n",
    "       'sentiment', 'emoji_count', 'public_metrics.retweet_count', 'viral']]\n",
    "\n",
    "combined_train = pd.concat([after, before])\n",
    "print(combined_train.shape)\n",
    "\n",
    "comb_X = combined_train[['verified', 'log_followers', 'log_following', 'log_tweetcount',\n",
    "       'log_listed', 'account_age_y', 'sex_generalized', 'tweet_char_len',\n",
    "       'hashtag_count', 'mention_count', 'urls_count', 'organization',\n",
    "       'sentiment', 'emoji_count', 'public_metrics.retweet_count']]\n",
    "\n",
    "comb_Y = combined_train['viral']\n",
    "\n",
    "sample = {0: 586497, 1: 6376}\n",
    "resample = RandomUnderSampler(random_state = 42, sampling_strategy = sample)\n",
    "comb_X, comb_Y = resample.fit_resample(comb_X, comb_Y)\n",
    "Counter(comb_Y)\n",
    "\n",
    "combined_data_RUS = comb_X.reset_index()\n",
    "combined_data_RUS['viral'] = comb_Y\n",
    "\n",
    "print('number of train data after RUS:', len(combined_data_RUS))\n",
    "\n",
    "resample_data_comb = combined_data_RUS[(combined_data_RUS['public_metrics.retweet_count'] < 25) | (combined_data_RUS['public_metrics.retweet_count'] > 100)]\n",
    "print('after resampling with bound of 25: ', len(resample_data_comb))\n",
    "\n",
    "comb_X = resample_data_comb[['verified', 'log_followers',\n",
    "       'log_following', 'log_tweetcount',\n",
    "       'log_listed', 'account_age_y', \n",
    "       'sex_generalized', 'tweet_char_len', \n",
    "        'hashtag_count',\n",
    "       'mention_count', 'urls_count', 'organization', 'sentiment', 'emoji_count']]\n",
    "\n",
    "comb_Y = resample_data_comb['viral']\n",
    "\n",
    "print(comb_X.shape, Counter(comb_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "RF = RandomForestClassifier(criterion = 'entropy', max_depth = 40, max_features = 'sqrt', min_samples_split = 5, n_estimators = 150, random_state = 42, n_jobs = 3)\n",
    "model_comb = RF.fit(comb_X, comb_Y)\n",
    "print(\"done fitting model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get F1 distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1_distr_comb = get_F1_distribution(comb_test, model_comb)\n",
    "plt.hist(F1_distr_comb, bins = 50)\n",
    "print(\"number of f-measures: \", len(F1_distr_comb))\n",
    "print(np.mean(F1_distr_comb), np.std(F1_distr_comb), stats.sem(F1_distr_comb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_distr_comb_after = get_F1_distribution(after_test, model_comb)\n",
    "plt.hist(F1_distr_comb_after, bins = 50)\n",
    "print(\"number of f-measures: \", len(F1_distr_comb_after))\n",
    "print(np.mean(F1_distr_comb_after), np.std(F1_distr_comb_after), stats.sem(F1_distr_comb_after))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHECK ALL MODEL PERFORMANCES FIGURES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_F1['CC'] = F1_distr_comb\n",
    "pd.DataFrame.from_dict(all_F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1_diff_B = list()\n",
    "F1_diff_A = list()\n",
    "F1_diff_A_C = list()\n",
    "F1_diff_B_C = list()\n",
    "\n",
    "for i in range(0,len(all_F1['BB'])):\n",
    "    bb = all_F1['BB'][i]\n",
    "    ba = all_F1['BA'][i]\n",
    "    F1_diff_B.append(bb - ba)\n",
    "    \n",
    "    aa = all_F1['AA'][i]\n",
    "    ab = all_F1['AB'][i]\n",
    "    F1_diff_A.append(aa - ab)\n",
    "    \n",
    "    cc = all_F1['CC'][i]\n",
    "    F1_diff_A_C.append(aa - cc)\n",
    "    F1_diff_B_C.append(bb - cc)\n",
    "    \n",
    "F1_differences = {1 : F1_diff_B, 2 : F1_diff_A, 3 : F1_diff_B_C, 4 : F1_diff_A_C}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "fig, axs = plt.subplots(2, 2, figsize = (12,8))\n",
    "counter = 1\n",
    "title = {1 : \"1. BB-BA\", 2 : \"2. AA-AB\", 3 : \"3. BB-CC\", 4 : \"4. AA-CC\"}\n",
    "\n",
    "for row in [0,1]:\n",
    "    for column in [0,1]:\n",
    "        \n",
    "        q1 = np.percentile(F1_differences[counter], 1.25)\n",
    "        q2 = np.percentile(F1_differences[counter], 98.75)\n",
    "        print(title[counter])\n",
    "        print(q1, q2)\n",
    "        \n",
    "        if counter < 3:\n",
    "            color = 'g'\n",
    "        else:\n",
    "            color = 'b'\n",
    "        axs[row, column].hist(F1_differences[counter], bins = 50, color = color)\n",
    "        axs[row, column].set_title(title[counter], fontsize = 13, weight = 'bold', loc = 'right')\n",
    "        axs[row, column].set_xlim(-0.04, 0.07)\n",
    "        axs[row, column].set_ylim(0, 70)\n",
    "        axs[row, column].axvline(color = '0', linestyle = 'dashed')\n",
    "        axs[row, column].axvline(q1, color = 'r')\n",
    "        axs[row, column].axvline(q2, color = 'r')\n",
    "        #         axs[row, column].set_ylim(0, 0.3)\n",
    "        \n",
    "        counter = counter + 1\n",
    "axs[0, 0].set_ylabel('Count', fontsize = 12)  \n",
    "axs[1, 0].set_ylabel('Count', fontsize = 12)  \n",
    "axs[1, 0].set_xlabel('Macro F1-score difference', fontsize = 12)\n",
    "axs[1, 1].set_xlabel('Macro F1-score difference', fontsize = 12)\n",
    "fig.tight_layout()\n",
    "# fig.savefig(\"difference_CI_performance.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
